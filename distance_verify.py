import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from art.attacks.evasion import ProjectedGradientDescent,FastGradientMethod,BasicIterativeMethod,SaliencyMapMethod
from art.estimators.classification import PyTorchClassifier
import os
import random
import time
import datetime
from collections import Counter, defaultdict
from sklearn.metrics import classification_report
from mytoolfunction import ChooseUseModel, getStartorEndtime
from adeversarial_config import SettingAderversarialConfig
from colorama import Fore, Back, Style, init
import math

# å®šç¾©å…©å€‹å‘é‡
# vector1 = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0],[5.0,5.0,5.0]])
# vector2 = torch.tensor([[2.0, 3.0, 4.0, 5.0, 6.0],[3.0,4.0,6.0]])

# å®šç¾©å…©å€‹å‘é‡
vector1 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 8.0]])
vector2 = torch.tensor([[4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])

print("Vector1:", vector1)
print("Vector2:", vector2)

# 1. è¨ˆç®—å–®å€‹å‘é‡çš„ L2 ç¯„æ•¸(å³é•·åº¦)
print("\n1. è¨ˆç®— Vector1 çš„ L2 ç¯„æ•¸:")

# è¨ˆç®—æ¯å€‹å…ƒç´ å¹³æ–¹çš„ç¸½å’Œï¼Œç„¶å¾Œå–å¹³æ–¹æ ¹
squared_sum1 = torch.sum(vector1*vector1)  # å°‡æ¯å€‹å…ƒç´ å¹³æ–¹ï¼Œç„¶å¾Œè¨ˆç®—ç¸½å’Œ
norm1 = math.sqrt(squared_sum1.item())  # å°‡ç¸½å’Œè½‰ç‚ºæ¨™é‡ä¸¦å–å¹³æ–¹æ ¹
print(f"æ‰‹å‹•è¨ˆç®—: âˆš({squared_sum1.item():.4f}) = {norm1:.4f}")

# è¨ˆç®—æ¯å€‹å…ƒç´ å¹³æ–¹çš„ç¸½å’Œï¼Œç„¶å¾Œå–å¹³æ–¹æ ¹
squared_sum2 = torch.sum(vector2*vector2)  # å°‡æ¯å€‹å…ƒç´ å¹³æ–¹ï¼Œç„¶å¾Œè¨ˆç®—ç¸½å’Œ
norm2 = math.sqrt(squared_sum2.item())  # å°‡ç¸½å’Œè½‰ç‚ºæ¨™é‡ä¸¦å–å¹³æ–¹æ ¹
print(f"æ‰‹å‹•è¨ˆç®—: âˆš({squared_sum2.item():.4f}) = {norm2:.4f}")


# ä½¿ç”¨ torch.norm() ä¾†è¨ˆç®— L2 ç¯„æ•¸
print(f"ä½¿ç”¨ torch.norm(): {torch.norm(vector1).item():.4f}")
print(f"ä½¿ç”¨ torch.norm(): {torch.norm(vector2).item():.4f}")


# 2. è¨ˆç®—å…©å€‹å‘é‡ä¹‹é–“çš„æ­å¹¾é‡Œå¾—è·é›¢
print("\n2. è¨ˆç®— Vector1 å’Œ Vector2 ä¹‹é–“çš„æ­å¹¾é‡Œå¾—è·é›¢:")

# è¨ˆç®—å…©å€‹å‘é‡ä¹‹é–“çš„å·®ç•°
diff = vector2 - vector1
# è¨ˆç®—å·®ç•°çš„å¹³æ–¹
squared_diff = diff*diff
# è¨ˆç®—å·®ç•°å¹³æ–¹å’Œ
squared_sum_diff = torch.sum(squared_diff)
# è¨ˆç®—å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ï¼Œå¾—åˆ°æ­å¹¾é‡Œå¾—è·é›¢
distance = math.sqrt(squared_sum_diff.item())

print("å·®ç•°å‘é‡:", diff)
print("å·®ç•°å¹³æ–¹:", squared_diff)
print(f"å¹³æ–¹å’Œ: {squared_sum_diff.item():.4f}")
print(f"æ­å¹¾é‡Œå¾—è·é›¢: âˆš{squared_sum_diff.item():.4f} = {distance:.4f}")
print(f"ä½¿ç”¨ torch.norm()æ±‚æ­å¹¾é‡Œå¾—è·é›¢: {torch.norm(vector1 - vector2,p=2).item():.4f}")


# å®šç¾©å…©å€‹å‘é‡
# vector_1 = torch.tensor([1.0, 2.0, 3.0])
# vector_2 = torch.tensor([4.0, 5.0, 6.0])

# è¨ˆç®—æ­å¹¾é‡Œå¾—è·é›¢
euclidean_distance = torch.norm(vector1 - vector2,p=2)

# è¨ˆç®—L2ç¯„æ•¸ (æ¨™æº–åŒ–)
normalized_vector_1 = vector1 / torch.norm(vector1,p=2)
normalized_vector_2 = vector2 / torch.norm(vector2,p=2)

# è¨ˆç®—å…©å€‹ç¶“éL2æ­£è¦åŒ–çš„å‘é‡ä¹‹é–“çš„æ­å¹¾é‡Œå¾—è·é›¢
normalized_distance = torch.norm(normalized_vector_1 - normalized_vector_2,p=2)

# è¼¸å‡ºçµæœ
print("Euclidean Distance:", euclidean_distance)
print("Normalized Euclidean Distance:", normalized_distance)

print("vector1:", vector1)

print("normalized_vector_1:", normalized_vector_1)

def ema_smooth(data, smoothing_factor=0.1):
    # Initialize EMA with the first data point
    smoothed_data = torch.zeros_like(data)
    smoothed_data[0] = data[0]

    # Apply EMA smoothing for each data point
    for i in range(1, len(data)):
        smoothed_data[i] = smoothing_factor * data[i] + (1 - smoothing_factor) * smoothed_data[i - 1]
    
    return smoothed_data

# Example usage
data = torch.tensor([0.1, 0.15, 0.2, 0.18, 0.25, 0.3, 0.5, 0.4, 0.35])  # Example data
smoothed_data = ema_smooth(data, smoothing_factor=0.1)

print(smoothed_data)
# Plot the original and smoothed data
# plt.figure(figsize=(10, 6))
# plt.plot(data.numpy(), label="Original Data", color='blue', linestyle='--', marker='o')
# plt.plot(smoothed_data.numpy(), label="Smoothed Data (EMA)", color='red', linewidth=2)
# plt.title("Original vs Smoothed Data (EMA)")
# plt.xlabel("Iterations")
# plt.ylabel("Value")
# plt.legend()
# plt.grid(True)
# plt.show()
# # 3. è¨ˆç®—å‘é‡çš„ L1 ç¯„æ•¸ï¼ˆæ›¼å“ˆé “è·é›¢ï¼‰
# print("\n3. è¨ˆç®— Vector1 çš„ L1 ç¯„æ•¸:")
# l1_norm1 = sum(abs(x) for x in vector1)
# print(f"æ‰‹å‹•è¨ˆç®—: {' + '.join(f'|{x}|' for x in vector1)} = {l1_norm1:.4f}")
# print(f"ä½¿ç”¨ torch.norm(p=1): {torch.norm(vector1, p=1).item():.4f}")

# # 4. è¨ˆç®—å…©å€‹å‘é‡ä¹‹é–“çš„ L1 è·é›¢
# print("\n4. è¨ˆç®— Vector1 å’Œ Vector2 ä¹‹é–“çš„ L1 è·é›¢:")
# l1_distance = sum(abs(x) for x in diff)
# print(f"æ‰‹å‹•è¨ˆç®—: {' + '.join(f'|{x:.2f}|' for x in diff)} = {l1_distance:.4f}")
# print(f"ä½¿ç”¨ torch.norm(p=1): {torch.norm(vector1 - vector2, p=1).item():.4f}")
# æå–å‰126è¡Œæ•¸æ“š
df = pd.read_csv("E:\\develop_Federated_Learning_Non_IID_Lab\\FL_AnalyseReportfolder\\20250319\\CICIDS2017_use_20250205_data_merge_label_FGSM_eps0.05æ¸¬è©¦_79_feature\\Inital_Local_weight_diff_client1_count.csv")
# list_
# list_ = df["dis_variation_Inital_Local"].head(10)
# list_ = df["dis_variation_Inital_Local"]

# ç´¯åŠ å¹³å‡å€¼è¨ˆç®—
# å¾ç¬¬11å€‹æ•¸å­—é–‹å§‹è¨ˆç®—ç´¯åŠ å¹³å‡å€¼ï¼ˆå¿½ç•¥å‰10å€‹æ•¸å­—ï¼‰
# å…ˆéæ¿¾æ‰å‰10å€‹æ•¸å­—
df_filtered = df["dis_variation_Inital_Local"][10:]

# å†éæ¿¾æ‰ç¬¬125åˆ°ç¬¬200å€‹æ•¸å­—ï¼ˆæ³¨æ„ç´¢å¼•å¾10é–‹å§‹ï¼Œæ‰€ä»¥éœ€è¦èª¿æ•´ç´¢å¼•ä½ç½®ï¼‰
# df_filtered = df_filtered.drop(df_filtered.index[124-10:199-10])
# è¨ˆç®—å‰©é¤˜æ•¸æ“šçš„ç´¯åŠ å¹³å‡å€¼
df["cumulative_mean"] = df_filtered.expanding().mean()

# è¨ˆç®—æ¯æ¬¡è¨ˆç®—çš„æœ€å¤§ç´¯åŠ å¹³å‡å€¼
df["cumulative_mean_max"] = df["cumulative_mean"].expanding().max()
# print("\ncumulative_mean:")
# print(df)


# è¨ˆç®—æ¯æ¬¡è¨ˆç®—å¾Œçš„æœ€å¤§ç´¯ç©å¹³å‡å€¼
df["cumulative_mean_max"] = df["cumulative_mean"].expanding().max()

# è¨ˆç®—æœ€å¤§ç´¯ç©å¹³å‡å€¼èˆ‡ç´¯ç©å¹³å‡å€¼ä¹‹é–“çš„å·®ç•°
df["difference"] = df["cumulative_mean_max"] - df["cumulative_mean"]

# è¨ˆç®—æœ€å¤§ç´¯ç©å¹³å‡å€¼èˆ‡ç´¯ç©å¹³å‡å€¼ä¹‹é–“çš„å·®ç•°å¾Œæ±‚å¹³æ–¹
df["squared_difference"] = df["difference"] ** 2

# è¨ˆç®—æ¯æ¬¡è¨ˆç®—å¾Œçš„æœ€å¤§ squared_difference
df["max_squared_difference"] = df["squared_difference"].expanding().max()

# ä¿®æ”¹å…¬å¼:Delta=ğ‘âˆ—max_squared_difference*K+(1-a)*Dç‚ºè¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„aå€¼0.5æˆ–0.1æ ¹æ“šè·é›¢è®ŠåŒ–é‡ï¼Œçµ¦å®šå€¼2æˆ–æ˜¯1
# è¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„aå€¼0.1 D=1
# è¿ªåˆ©å…‹é›·åˆ†å¸ƒçš„aå€¼0.5 D=2

#  è¨ˆç®— scale_factor
# scale_factor = df["max_squared_difference"] / df["cumulative_mean_max"] 
# df["scale_factor"] = df.apply(lambda row: max(1, row['max_squared_difference'] / row['cumulative_mean_max']) * 3, axis=1)
# df["scale_factor"] = max(1, scale_factor ) * 3
# *kå€
df["Delta_K_2"]=0.5*df["max_squared_difference"]*2+0.5*1
df["Delta_K_3"]=0.5*df["max_squared_difference"]*3+0.5*1
df["Delta_K_4"]=0.5*df["max_squared_difference"]*4+0.5*1
# df["Delta_K_5"]=0.5*df["max_squared_difference"]*5+0.50.1*2
# df["Delta_a=0.1"]=0.5*df["max_squared_difference"]*2+0.5*1
# df["Delta_a=0.5"]=0.5*df["max_squared_difference"]*2+0.5+0.5*2
# df["Delta"]=0.5*df["max_squared_difference"]*2+0.1*2

# é¡¯ç¤ºçµæœ
print("\ncumulative_mean, cumulative_mean_max, difference, squared_difference, max_squared_difference:")
print(df)
df.to_csv("./cumulative_mean_max-cumulative_mean_max_teset.csv")



# # è¨ˆç®—EMAï¼Œä½¿ç”¨spanä¾†æ§åˆ¶å¹³æ»‘ç¨‹åº¦
# df['EMA'] = df['dis_variation_Inital_Local'].ewm(span=3, adjust=False).mean()
# # df['EMA'] = df['dis_variation_Inital_Local'].rolling(window=3).mean()  # ç®€å•æ»‘åŠ¨çª—å£æ–¹æ³•

# # ä½¿ç”¨rolling + applyæ¥å®ç°WMA
# weights = [0.5,0.3,0.2]  # æƒé‡å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
# df['WMA'] = df['dis_variation_Inital_Local'].rolling(window=3).apply(lambda x: (x * weights).sum() / sum(weights), raw=False)
# # æŸ¥çœ‹è¨ˆç®—çµæœ
# print(df[['dis_variation_Inital_Local', 'EMA']].head(20))  # é¡¯ç¤ºå‰20è¡Œçš„çµæœ

# # å¦‚æœä½ åªæƒ³æŸ¥çœ‹EMAçš„çµæœ
# print(df['EMA'])  # é¡¯ç¤ºå‰20è¡Œçš„EMAçµæœ
# df['EMA'].to_csv("./EMA.csv")
# df['WMA'].to_csv("./WMA.csv")

# # ç¹ªè£½åœ–è¡¨
# plt.figure(figsize=(10,6))
# plt.plot(df['dis_variation_Inital_Local'], label='Original Data', color='blue', alpha=0.5)
# plt.plot(df['EMA'], label='Exponential Moving Average (EMA)', color='orange', linestyle='--')
# plt.plot(df['WMA'], label='Exponential Moving Average (WMA)', color='red', linestyle='-')
# # æ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤
# plt.title('Exponential Moving Average (EMA) of dis_variation_Inital_Local', fontsize=16)
# plt.xlabel('Rounds', fontsize=12)
# plt.ylabel('Value', fontsize=12)

# # æ·»åŠ åœ–ä¾‹
# plt.legend()

# # é¡¯ç¤ºåœ–è¡¨
# plt.show()